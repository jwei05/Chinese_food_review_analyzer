{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese food review analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sentiment analysis in Chinese food review data\n",
    "\n",
    "The architecture for the neural network and parts of the preprocessing (i.e. tokenization) of this Chinese sentiment analysis are borrowed from https://github.com/Tony607/Chinese_sentiment_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import codecs\n",
    "from langconv import * # convert Traditional Chinese characters to Simplified Chinese characters\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get files and Prepare doucments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeReviews = './data/reviews/neg/both_neg.txt'\n",
    "positiveReviews = './data/reviews/pos/both_pos.txt'\n",
    "\n",
    "documents = []\n",
    "with codecs.open(positiveReviews, \"rb\") as doc_file:\n",
    "    for line in doc_file:\n",
    "        try:\n",
    "            line = line.decode(\"utf-8\")\n",
    "        except:\n",
    "            continue\n",
    "        # Convert from traditional to simplified Chinese\n",
    "        text = Converter('zh-hans').convert(line)\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = text.replace(\"\\r\", \"\")\n",
    "        documents.append((text, \"pos\"))\n",
    "\n",
    "with codecs.open(negativeReviews, \"rb\") as doc_file:\n",
    "    for line in doc_file:\n",
    "        try:\n",
    "            line = line.decode(\"utf-8\")\n",
    "        except:\n",
    "            continue\n",
    "        # Convert from traditional to simplified Chinese\n",
    "        text = Converter('zh-hans').convert(line)\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = text.replace(\"\\r\", \"\")\n",
    "        documents.append((text, \"neg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data\n",
    "\n",
    "Because we don't want all the positives and then all the negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input and output for the model\n",
    "Each review will be a list of tokens, output will be one token(\"pos\" or \"neg\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/2m/qy2t9vkn71g_l16mj4nxvbwc0000gn/T/jieba.cache\n",
      "Loading model cost 1.094 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "totalX = []\n",
    "totalY = [str(doc[1]) for doc in documents]\n",
    "for doc in documents:\n",
    "    seg_list = jieba.cut(doc[0], cut_all=False)\n",
    "    seg_list = list(seg_list)\n",
    "    totalX.append(seg_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine max length\n",
    "Decide the max input sequence, here we cover up to 60% sentences. The longer input sequence, the more training time will take, but could improve  prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = sorted([len(sentence) for sentence in totalX])\n",
    "print(h)\n",
    "maxLength = h[int(len(h) * 0.60)]\n",
    "print(\"Max length is: \",h[-1])\n",
    "print(\"60% cover length up to: \",maxLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words to number tokens, padding\n",
    "Pad input sequence to max input length if it is shorter\n",
    "\n",
    "\n",
    "Save the input tokenizer, since we need to use the same tokenizer for our new predition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalX = [\" \".join(wordslist) for wordslist in totalX]  # Keras Tokenizer expect the words tokens to be seperated by space \n",
    "input_tokenizer = Tokenizer(30000) # Initial vocab size\n",
    "input_tokenizer.fit_on_texts(totalX)\n",
    "vocab_size = len(input_tokenizer.word_index) + 1\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX), maxlen=maxLength))\n",
    "__pickleStuff(\"./data/input_tokenizer_chinese.p\", input_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocab_size: 3861\n"
     ]
    }
   ],
   "source": [
    "print(\"input vocab_size:\",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0  28  54   6  21  21  21\n",
      " 157  67   4  11 412   2 179  78   2  70   9   4  11]\n"
     ]
    }
   ],
   "source": [
    "print(totalX[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output, array of 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output vocab_size: 3\n"
     ]
    }
   ],
   "source": [
    "target_tokenizer = Tokenizer(3) \n",
    "target_tokenizer.fit_on_texts(totalY)\n",
    "print(\"output vocab_size:\",len(target_tokenizer.word_index) + 1)\n",
    "totalY = np.array(target_tokenizer.texts_to_sequences(totalY)) -1\n",
    "totalY = totalY.reshape(totalY.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(totalY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn output 0s and 1s to categories(one-hot vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalY = to_categorical(totalY, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalY[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dimen = totalY.shape[1] # which is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save meta data for later predition\n",
    "maxLength: the input sequence length\n",
    "\n",
    "vocab_size: Input vocab size\n",
    "\n",
    "output_dimen: which is 2 in this example (pos or neg)\n",
    "\n",
    "sentiment_tag: either [\"neg\",\"pos\"] or [\"pos\",\"neg\"] matching the target tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reverse_word_index = {v: k for k, v in list(target_tokenizer.word_index.items())}\n",
    "sentiment_tag = [target_reverse_word_index[1],target_reverse_word_index[2]] \n",
    "metaData = {\"maxLength\":maxLength,\"vocab_size\":vocab_size,\"output_dimen\":output_dimen,\"sentiment_tag\":sentiment_tag}\n",
    "__pickleStuff(\"./data/meta_sentiment_chinese.p\", metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model, train and save it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 31, 256)           988416    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 31, 256)           393984    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,776,898\n",
      "Trainable params: 1,776,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 623 samples, validate on 70 samples\n",
      "Epoch 1/20\n",
      "623/623 [==============================] - 7s 11ms/step - loss: 0.6648 - acc: 0.6260 - val_loss: 0.6191 - val_acc: 0.6714\n",
      "Epoch 2/20\n",
      "623/623 [==============================] - 5s 8ms/step - loss: 0.6546 - acc: 0.6292 - val_loss: 0.6073 - val_acc: 0.6714\n",
      "Epoch 3/20\n",
      "623/623 [==============================] - 5s 7ms/step - loss: 0.6406 - acc: 0.6244 - val_loss: 0.5755 - val_acc: 0.7000\n",
      "Epoch 4/20\n",
      "623/623 [==============================] - 6s 10ms/step - loss: 0.6104 - acc: 0.6774 - val_loss: 0.5125 - val_acc: 0.7429\n",
      "Epoch 5/20\n",
      "623/623 [==============================] - 4s 7ms/step - loss: 0.5532 - acc: 0.7303 - val_loss: 0.4405 - val_acc: 0.7857\n",
      "Epoch 6/20\n",
      "623/623 [==============================] - 5s 8ms/step - loss: 0.4445 - acc: 0.7994 - val_loss: 0.4375 - val_acc: 0.7857\n",
      "Epoch 7/20\n",
      "623/623 [==============================] - 4s 7ms/step - loss: 0.3805 - acc: 0.8379 - val_loss: 0.4015 - val_acc: 0.8143\n",
      "Epoch 8/20\n",
      "623/623 [==============================] - 4s 7ms/step - loss: 0.2938 - acc: 0.8636 - val_loss: 0.3971 - val_acc: 0.8286\n",
      "Epoch 9/20\n",
      "623/623 [==============================] - 4s 7ms/step - loss: 0.2238 - acc: 0.9213 - val_loss: 0.4433 - val_acc: 0.8000\n",
      "Epoch 10/20\n",
      "623/623 [==============================] - 5s 8ms/step - loss: 0.2349 - acc: 0.9117 - val_loss: 0.3886 - val_acc: 0.8429\n",
      "Epoch 11/20\n",
      "623/623 [==============================] - 5s 8ms/step - loss: 0.1973 - acc: 0.9454 - val_loss: 0.3821 - val_acc: 0.8571\n",
      "Epoch 12/20\n",
      "623/623 [==============================] - 5s 7ms/step - loss: 0.1275 - acc: 0.9551 - val_loss: 0.4193 - val_acc: 0.8429\n",
      "Epoch 13/20\n",
      "623/623 [==============================] - 5s 8ms/step - loss: 0.0936 - acc: 0.9695 - val_loss: 0.4774 - val_acc: 0.8571\n",
      "Epoch 14/20\n",
      "623/623 [==============================] - 6s 10ms/step - loss: 0.1164 - acc: 0.9631 - val_loss: 0.4570 - val_acc: 0.8714\n",
      "Epoch 15/20\n",
      "623/623 [==============================] - 4s 7ms/step - loss: 0.0929 - acc: 0.9615 - val_loss: 0.4463 - val_acc: 0.8714\n",
      "Epoch 16/20\n",
      "623/623 [==============================] - 4s 7ms/step - loss: 0.0899 - acc: 0.9743 - val_loss: 0.4870 - val_acc: 0.8286\n",
      "Epoch 17/20\n",
      "623/623 [==============================] - 4s 7ms/step - loss: 0.0466 - acc: 0.9839 - val_loss: 0.5699 - val_acc: 0.8429\n",
      "Epoch 18/20\n",
      "623/623 [==============================] - 4s 7ms/step - loss: 0.0509 - acc: 0.9823 - val_loss: 0.6605 - val_acc: 0.8571\n",
      "Epoch 19/20\n",
      "623/623 [==============================] - 5s 8ms/step - loss: 0.0757 - acc: 0.9791 - val_loss: 0.6509 - val_acc: 0.8429\n",
      "Epoch 20/20\n",
      "623/623 [==============================] - 4s 7ms/step - loss: 0.0399 - acc: 0.9888 - val_loss: 0.6661 - val_acc: 0.8429\n",
      "Saved model!\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,input_length = maxLength))\n",
    "# Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "# All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "# Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "model.add(GRU(256, dropout=0.9))\n",
    "# The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "model.add(Dense(output_dimen, activation='softmax'))\n",
    "# We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "tbCallBack = TensorBoard(log_dir='./Graph/sentiment_chinese', histogram_freq=0,\n",
    "                            write_graph=True, write_images=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(totalX, totalY, validation_split=0.1, batch_size=32, epochs=20, verbose=1, callbacks=[tbCallBack])\n",
    "! rm mod.h5\n",
    "model.save('mod.h5')\n",
    "print(\"Saved model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading meta data and the model we just trained and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "    \n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff\n",
    "\n",
    "#Function to load model + meta data\n",
    "def load_it(filepath):\n",
    "    global model, sentiment_tag, maxLength\n",
    "    metaData = __loadStuff(\"./data/meta_sentiment_chinese.p\")\n",
    "    maxLength = metaData.get(\"maxLength\")\n",
    "    vocab_size = metaData.get(\"vocab_size\")\n",
    "    output_dimen = metaData.get(\"output_dimen\")\n",
    "    sentiment_tag = metaData.get(\"sentiment_tag\")\n",
    "    embedding_dim = 256\n",
    "    model = load_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sentence to model input\n",
    "def findFeatures(text):\n",
    "    text=Converter('zh-hans').convert(text)\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\") \n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    seg_list = list(seg_list)\n",
    "    text = \" \".join(seg_list)\n",
    "    textArray = [text]\n",
    "    input_tokenizer_load = __loadStuff(\"./data/input_tokenizer_chinese.p\")\n",
    "    textArray = np.array(pad_sequences(input_tokenizer_load.texts_to_sequences(textArray), maxlen=maxLength))\n",
    "    return textArray\n",
    "\n",
    "# Function to predict result\n",
    "def predictResult(text):\n",
    "    features = findFeatures(text)\n",
    "    predicted = model.predict(features)[0] # we have only one sentence to predict, so take index 0\n",
    "    predicted = np.array(predicted)\n",
    "    probab = predicted.max()\n",
    "    predition = sentiment_tag[predicted.argmax()]\n",
    "    return predition, probab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_it('mod.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.9678621)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"姜葱焗龙虾是几乎每桌必点的招牌菜，分为两种规格，大龙虾是一只2.5磅以上的大龙虾，当天时价16.95刀/磅，双龙虾是两只约1磅的小龙虾（共计2磅左右），当天时价29.95刀/份，点菜时服务员直接推荐了按份计价，相对实惠的双龙虾，上菜后感觉菜量并不小，三个人吃还是可以的。龙虾的肉质鲜嫩饱满，葱姜炒的做法对足了中国人的胃口，吃着格外舒坦，尽管勾芡稍有些稠，味道也稍微偏咸，但瑕不掩瑜，依旧吃得很过瘾。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neg', 0.99999464)\n"
     ]
    }
   ],
   "source": [
    "print(predictResult(\"蒜蓉通菜上菜很快，菜量很大，摆盘略丑，通心菜有些老，色泽略微偏黄，炒得火候比较老，口感偏软，味道也比较一般。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pos', 0.999877)\n"
     ]
    }
   ],
   "source": [
    "print(predictResult(\"瑶柱蛋白炒饭上菜较快，菜量较大，米饭炒得颗粒分明，咸淡适中，葱花和干贝丝点缀其中，鲜而不咸，吃起来很香。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pos', 0.94182307)\n"
     ]
    }
   ],
   "source": [
    "print(predictResult(\"鱼香肉丝超级好吃的\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Menu\n",
    "hard-coded now but can be more dynamic (pdf converter, html scraper, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu = {}\n",
    "menu[\"三文鱼\"] = \"Salmon\"\n",
    "menu[\"茶碗蒸\"] = \"Steamed Egg\"\n",
    "menu[\"黄瓜鳗鱼卷\"] = \"UNAGI KYURI MAKI\"\n",
    "menu[\"波龙\"] = \"Lobster with Ginger and Scallion\"\n",
    "menu[\"清蒸鱼\"] = \"Steamed Big Flounder\"\n",
    "menu[\"鱼香肉丝\"] = \"Shredded Pork with Garlic Sauce\"\n",
    "menu[\"宫保鸡丁\"] = \"Kung Pao Chicken\"\n",
    "menu[\"姜葱焗龙虾\"] = \"Lobster with Ginger and Scallion\"\n",
    "menu[\"蒜蓉通菜\"] = \"Stir Fried Tong Cai\"\n",
    "menu[\"瑶柱蛋白炒饭\"] = \"Dried Scallop with Egg White\"\n",
    "menu[\"红烧豆腐\"] = \"Fried Bean Curd with Mushroom\"\n",
    "menu[\"蚝油牛肉\"] = \"Beef with Oyster Sauce\"\n",
    "menu[\"生炒三鲜\"] = \"Pan Fried Assorted Seafood\"\n",
    "menu[\"节瓜粉丝煲\"] = \"Dried Shrimp, Vermicelli and Pork with Fuzzy Melon\"\n",
    "menu[\"红豆汤\"] = \"Red Bean Soup\"\n",
    "menu[\"西米露\"] = \"Sweet Milk with Boba\"\n",
    "menu[\"椒盐牛仔骨\"] = \"Spicy Salted Dry Fried Veal Ribs\"\n",
    "menu[\"红酒烩牛尾\"] = \"Ox Tail with Red Wine\" \n",
    "menu[\"椒盐龙虾\"] = \"Spicy Salted Dry Fried Lobster\"\n",
    "menu[\"罗汉斋\"] = \"Buddha's Delight\"\n",
    "menu[\"珊瑚虾刺身\"]= \"Shrimp Sashimi\"\n",
    "menu[\"椒盐大蟹\"] = \"Spicy Salted Dry Fried Crab\"\n",
    "menu[\"豉汁蒸生蚝\"] = \"Steamed Oysters with Black Bean Sauce\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parser \n",
    "Segments a real review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file\n",
    "Save file into \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review2 = './data/reviews/test_review2.txt'\n",
    "filename = test_review2\n",
    "with codecs.open(filename, \"rb\") as doc_file:\n",
    "    count = 0\n",
    "    for line in doc_file:\n",
    "        count += 1\n",
    "        try:\n",
    "            line = line.decode(\"utf-8\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "text = \"\"\n",
    "with codecs.open(filename, \"r\") as doc_file:\n",
    "    for line in doc_file:\n",
    "        text += line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the mapping between dishes and reviews\n",
    "\n",
    "The reviews are later fed into the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dishes(phrase, menu):\n",
    "    dishes = set()\n",
    "    for d in menu:\n",
    "        if d in phrase:\n",
    "            dishes.add(d)\n",
    "    return dishes\n",
    "\n",
    "def contains_diff_dish(phrase, dishes, menu):\n",
    "    for d in menu:\n",
    "        if d in phrase and d not in dishes:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def create_mapping(review, menu):\n",
    "    mapping = dict()\n",
    "    sentences = review.split(\"。\")\n",
    "    for sentence in sentences:\n",
    "        phrases = sentence.split(\"，\")\n",
    "        i = 0\n",
    "        while i < len(phrases):\n",
    "            phrase = phrases[i]\n",
    "            dishes = list(get_dishes(phrase, menu))\n",
    "            if len(dishes) >= 1:\n",
    "                idea = phrase\n",
    "                i += 1\n",
    "                while i < (len(phrases)) and not contains_diff_dish(phrases[i], dishes, menu):\n",
    "                    idea += \"，\"\n",
    "                    idea += phrases[i]\n",
    "                    i+=1\n",
    "                for d in dishes:\n",
    "                    mapping.update({d: idea})\n",
    "            else:\n",
    "                i+=1\n",
    "    return mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'姜葱焗龙虾': '姜葱焗龙虾是几乎每桌必点的招牌菜，分为两种规格，大龙虾是一只2.5磅以上的大龙虾，当天时价16.95刀/磅，双龙虾是两只约1磅的小龙虾（共计2磅左右），当天时价29.95刀/份，点菜时服务员直接推荐了按份计价，相对实惠的双龙虾，上菜后感觉菜量并不小，三个人吃还是可以的',\n",
       " '蒜蓉通菜': '蒜蓉通菜和鱼香肉丝上菜很快，菜量很大，摆盘略丑，通心菜有些老，色泽略微偏黄，炒得火候比较老，口感偏软，味道也比较一般',\n",
       " '鱼香肉丝': '蒜蓉通菜和鱼香肉丝上菜很快，菜量很大，摆盘略丑，通心菜有些老，色泽略微偏黄，炒得火候比较老，口感偏软，味道也比较一般',\n",
       " '瑶柱蛋白炒饭': '瑶柱蛋白炒饭上菜较快，菜量较大，米饭炒得颗粒分明，咸淡适中，葱花和干贝丝点缀其中，鲜而不咸，吃起来很香'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = create_mapping(text, menu)\n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get results \n",
    "\n",
    "Combine all three parts.\n",
    "Run sentiment analysis on reviews for dishes, assign dishes to \"Good\", \"Bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good dishes: \n",
      "姜葱焗龙虾 -- Lobster with Ginger and Scallion\n",
      "瑶柱蛋白炒饭 -- Dried Scallop with Egg White\n",
      "Bad dishes: \n",
      "蒜蓉通菜 -- Stir Fried Tong Cai\n",
      "鱼香肉丝 -- Shredded Pork with Garlic Sauce\n"
     ]
    }
   ],
   "source": [
    "neg_list = []\n",
    "pos_list = []\n",
    "for dish_name in mapping:\n",
    "    review = mapping[dish_name]\n",
    "    if predictResult(review)[0] == \"neg\":\n",
    "        neg_list.append(dish_name)\n",
    "    else:\n",
    "        pos_list.append(dish_name)\n",
    "\n",
    "print(\"Good dishes: \")\n",
    "for dish in pos_list:\n",
    "    print(dish + \" -- \" + menu[dish])\n",
    "print(\"Bad dishes: \")\n",
    "for dish in neg_list:\n",
    "    print(dish + \" -- \" + menu[dish])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
